{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Attention Mechanism - Interactive Learning Notebook\n",
    "\n",
    "Welcome to the interactive attention mechanism tutorial! This notebook will guide you through implementing the core components of the attention mechanism step by step.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand and implement:\n",
    "1. **Linear Projections** for Query (Q), Key (K), and Value (V) matrices\n",
    "2. **Scaled Dot-Product Attention** computation\n",
    "3. **Softmax & Attention Weights** calculation\n",
    "4. **Value Aggregation** using attention weights\n",
    "\n",
    "## Example Prompt\n",
    "Throughout this tutorial, we'll use this consistent example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_EXAMPLE = \"The cat sat on the mat\"\n",
    "print(f\"Working with example: '{PROMPT_EXAMPLE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Notebook Navigation\n",
    "\n",
    "- **Required** *(Graded)*: Sections 1-4 + Final Evaluation\n",
    "- **Optional** *(Not Graded)*: Defined as \"Part B\" is optional for deeper understanding\n",
    "\n",
    "‚è±Ô∏è Estimated Time:\n",
    "- **Required sections:** 45-60 minutes\n",
    "- **Optional exploration:** 30+ minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.visualizations import (\n",
    "    visualize_qkv_projections,\n",
    "    visualize_attention_scores,\n",
    "    visualize_attention_weights,\n",
    "    visualize_attended_values\n",
    ")\n",
    "from src.model_utils import tokenize_text, create_embeddings\n",
    "from src.evaluation import evaluate_attention_output\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù REQUIRED: Section 1: Linear Projections (Q, K, V)\n",
    "\n",
    "## The Intuition: Three Perspectives on Information\n",
    "\n",
    "Imagine you're at a library looking for information about \"machine learning books.\" You would:\n",
    "1. **Ask** the librarian about books on machine learning (Query - what you're looking for)\n",
    "2. The librarian **checks** the catalog for available books (Keys - what's available to match)\n",
    "3. You **receive** the actual books that match (Values - the information you get)\n",
    "\n",
    "In attention mechanisms, we create these three \"perspectives\" for each word in our sentence.\n",
    "\n",
    "## Theory: Why Do We Need Q, K, V?\n",
    "\n",
    "The attention mechanism needs to answer: **\"For each word, which other words should it pay attention to?\"**\n",
    "\n",
    "Consider our example: **\"The cat sat on the mat\"**\n",
    "\n",
    "For the word \"cat\":\n",
    "- **Query (Q)**: \"What information does 'cat' need?\" ‚Üí Maybe it needs to know what action it's performing\n",
    "- **Key (K)**: \"What information can each word provide?\" ‚Üí \"sat\" can provide action information  \n",
    "- **Value (V)**: \"What is the actual information?\" ‚Üí The semantic meaning of \"sat\" (an action)\n",
    "\n",
    "### The Three Transformations\n",
    "\n",
    "Starting with the same input embeddings **X**, we create three different \"views\":\n",
    "\n",
    "- **Query (Q)**: *\"What am I looking for?\"* - Transforms input to represent information needs\n",
    "- **Key (K)**: *\"What can I provide?\"* - Transforms input to represent available information  \n",
    "- **Value (V)**: *\"What information do I actually contain?\"* - Transforms input to represent the content to be retrieved\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$Q = XW_Q$$\n",
    "$$K = XW_K$$  \n",
    "$$V = XW_V$$\n",
    "\n",
    "Where:\n",
    "- $X \\in \\mathbb{R}^{L \\times d_{model}}$: Input embeddings (sequence length √ó embedding dimension)\n",
    "- $W_Q, W_K, W_V \\in \\mathbb{R}^{d_{model} \\times d_k}$: Learned weight matrices (embedding dim √ó projection dim)\n",
    "- $Q, K, V \\in \\mathbb{R}^{L \\times d_k}$: Projected query, key, value matrices\n",
    "\n",
    "### Why Different Weight Matrices?\n",
    "\n",
    "Each weight matrix learns to extract different aspects:\n",
    "- $W_Q$: Learns to extract \"what information this position needs\"\n",
    "- $W_K$: Learns to extract \"what information this position can provide\" \n",
    "- $W_V$: Learns to extract \"the actual information content\"\n",
    "\n",
    "### Tensor Shape Deep Dive\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens):\n",
    "- Input embeddings: `(1, 6, 512)` ‚Üí 1 batch, 6 tokens, 512-dim embeddings\n",
    "- After projection: `(1, 6, 64)` ‚Üí 1 batch, 6 tokens, 64-dim projections\n",
    "\n",
    "The reduction from 512 to 64 dimensions serves two purposes:\n",
    "1. **Computational efficiency**: Smaller attention computations\n",
    "2. **Multiple heads**: We can have multiple attention heads in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: CORE ASSIGNMENT (GRADED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize example data\n",
    "PROMPT_EXAMPLE = \"The cat sat on the mat\"\n",
    "tokens = tokenize_text(PROMPT_EXAMPLE, method='word')  # Use word-level tokenization to match expected format\n",
    "embeddings = create_embeddings(tokens)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qkv_projections(embeddings, d_model=512, d_k=64):\n",
    "    \"\"\"\n",
    "    Create Query, Key, and Value projections from input embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Input embeddings tensor (batch_size, seq_len, d_model)\n",
    "        d_model: Dimension of input embeddings\n",
    "        d_k: Dimension of Q, K, V projections\n",
    "    \n",
    "    Returns:\n",
    "        Q, K, V: Query, Key, Value tensors (batch_size, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== REQUIRED IMPLEMENTATION (GRADED) =====\n",
    "    # TODO: Get input dimensions\n",
    "    batch_size, seq_len, embedding_dim = embeddings.shape\n",
    "    # TODO: Create linear projection layers\n",
    "    W_q = nn.Linear(embedding_dim, d_k, bias=False)\n",
    "    W_k = nn.Linear(embedding_dim, d_k, bias=False)\n",
    "    W_v = nn.Linear(embedding_dim, d_k, bias=False)\n",
    "    # TODO: Apply projections to input embedings\n",
    "    Q = W_q(embeddings)  # (batch_size, seq_len, d_k)\n",
    "    K = W_k(embeddings)  # (batch_size, seq_len, d_k)\n",
    "    V = W_v(embeddings)  # (batch_size, seq_len, d_k)\n",
    "    # ===== END REQUIRED SECTION =====\n",
    "    return Q, K, V\n",
    "\n",
    "# Test implementation\n",
    "Q, K, V = create_qkv_projections(embeddings)\n",
    "print(f\"Q shape: {Q.shape}, K shape: {K.shape}, V shape: {V.shape}\")\n",
    "print(\"Successfully created Q, K, V projections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "visualize_qkv_projections(embeddings, Q, K, V, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù REQUIRED: Section 2: Scaled Dot-Product Attention\n",
    "\n",
    "## The Intuition: Measuring Compatibility\n",
    "\n",
    "Think of this step as **matchmaking between questions and answers**:\n",
    "- Each Query asks: *\"What information do I need?\"*\n",
    "- Each Key responds: *\"Here's what I can provide\"*  \n",
    "- The dot product measures: *\"How well do they match?\"*\n",
    "\n",
    "### Why Dot Product for Similarity?\n",
    "\n",
    "The dot product between two vectors measures their **alignment**:\n",
    "- **High dot product**: Vectors point in similar directions ‚Üí High compatibility\n",
    "- **Low dot product**: Vectors are orthogonal ‚Üí Low compatibility  \n",
    "- **Negative dot product**: Vectors point in opposite directions ‚Üí Incompatible\n",
    "\n",
    "**Example**: If Query for \"cat\" is looking for \"action information\" and Key for \"sat\" provides \"action information\", their dot product will be high.\n",
    "\n",
    "### The Mathematical Operation\n",
    "\n",
    "$$\\text{Attention Scores} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "Let's break this down step by step:\n",
    "\n",
    "#### Step 1: Matrix Multiplication $QK^T$\n",
    "- $Q \\in \\mathbb{R}^{L \\times d_k}$: Each row is a query vector for one token\n",
    "- $K^T \\in \\mathbb{R}^{d_k \\times L}$: Each column is a key vector for one token  \n",
    "- Result: $\\mathbb{R}^{L \\times L}$ matrix where entry $(i,j)$ = similarity between token $i$'s query and token $j$'s key\n",
    "\n",
    "#### Step 2: Scaling by $\\sqrt{d_k}$\n",
    "\n",
    "**Why do we need scaling?**\n",
    "As the dimension $d_k$ increases, dot products tend to grow larger in magnitude. This pushes values toward the extremes of the softmax function where gradients become extremely small.\n",
    "\n",
    "**The Problem**: Without scaling, for $d_k = 512$:\n",
    "- Random dot products have variance ‚âà 512\n",
    "- Softmax becomes nearly deterministic (almost one-hot)\n",
    "- Gradients vanish during training\n",
    "\n",
    "**The Solution**: Dividing by $\\sqrt{d_k}$ normalizes the variance back to ‚âà 1\n",
    "\n",
    "### Tensor Shape Analysis\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens, $d_k = 64$):\n",
    "\n",
    "1. **Q shape**: `(1, 6, 64)` - 6 query vectors, each 64-dimensional\n",
    "2. **K shape**: `(1, 6, 64)` - 6 key vectors, each 64-dimensional  \n",
    "3. **K^T shape**: `(1, 64, 6)` - Transposed for matrix multiplication\n",
    "4. **QK^T shape**: `(1, 6, 6)` - 6√ó6 attention score matrix\n",
    "\n",
    "Each element `[i, j]` represents: *\"How much should token i attend to token j?\"*\n",
    "\n",
    "### Attention Score Matrix Interpretation\n",
    "\n",
    "For our example sentence, the 6√ó6 matrix might look like:\n",
    "```\n",
    "         The  cat  sat  on  the  mat\n",
    "    The  [ ?   ?    ?   ?   ?    ? ]\n",
    "    cat  [ ?   ?    ?   ?   ?    ? ]  \n",
    "    sat  [ ?   ?    ?   ?   ?    ? ]\n",
    "    on   [ ?   ?    ?   ?   ?    ? ]\n",
    "    the  [ ?   ?    ?   ?   ?    ? ]\n",
    "    mat  [ ?   ?    ?   ?   ?    ? ]\n",
    "```\n",
    "\n",
    "Higher scores indicate stronger relationships (e.g., \"cat\" ‚Üí \"sat\" for subject-verb relationship).\n",
    "\n",
    "### The Complete Formula Intuition\n",
    "\n",
    "$$\\text{Score}_{i,j} = \\frac{\\text{query}_i \\cdot \\text{key}_j}{\\sqrt{d_k}}$$\n",
    "\n",
    "This answers: *\"How relevant is the information that token j can provide to what token i is looking for?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_scores(Q, K):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention scores.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch_size, seq_len, d_k)\n",
    "        K: Key tensor (batch_size, seq_len, d_k)\n",
    "    \n",
    "    Returns:\n",
    "        attention_scores: Attention scores (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Implementation follows: scores = Q * K^T / sqrt(d_k)\n",
    "    \n",
    "    # ===== REQUIRED IMPLEMENTATION (GRADED) =====\n",
    "    # TODO: Get the dimension d_k for scaling\n",
    "    d_k = K.shape[-1]\n",
    "    # TODO: Compute dot product between Q and K^T\n",
    "    # Q: (batch_size, seq_len, d_k)\n",
    "    # K^T: (batch_size, d_k, seq_len)\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, seq_len, seq_len)\n",
    "    # TODO: Scale by sqrt(d_k) to prevent extreme values\n",
    "    attention_scores = attention_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    # ===== END REQUIRED SECTION =====\n",
    "    \n",
    "    return attention_scores\n",
    "\n",
    "# Test implementation\n",
    "attention_scores = compute_attention_scores(Q, K)\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(f\"d_k = {Q.shape[-1]}, scaling factor = {torch.sqrt(torch.tensor(Q.shape[-1], dtype=torch.float32)):.2f}\")\n",
    "print(\"Successfully computed attention scores!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "visualize_attention_scores(attention_scores, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù REQUIRED: Section 3: Softmax & Attention Weights\n",
    "\n",
    "## The Intuition: From Scores to Decisions\n",
    "\n",
    "Imagine you're deciding how to allocate your attention while reading \"The cat sat on the mat\":\n",
    "- You have **compatibility scores** for how relevant each word is\n",
    "- But you need to make a **decision**: How much attention to give each word?\n",
    "- Softmax converts raw scores into a **probability distribution** - a recipe for attention allocation\n",
    "\n",
    "### Why Convert to Probabilities?\n",
    "\n",
    "Raw attention scores can be any real numbers (positive, negative, large, small). We need:\n",
    "1. **Interpretability**: Weights between 0 and 1 are easier to understand\n",
    "2. **Normalization**: Weights sum to 1, so we're not \"over-attending\"  \n",
    "3. **Differentiability**: Smooth function for gradient-based learning\n",
    "\n",
    "### The Softmax Function\n",
    "\n",
    "$$\\text{Attention Weight}_{i,j} = \\frac{\\exp(\\text{Score}_{i,j})}{\\sum_{k=1}^{L} \\exp(\\text{Score}_{i,k})}$$\n",
    "\n",
    "**What this does**:\n",
    "- **Exponential**: $\\exp(x)$ makes all values positive and amplifies differences\n",
    "- **Normalization**: Division ensures weights sum to 1 for each query position\n",
    "- **Probability distribution**: Each row becomes a valid probability distribution\n",
    "\n",
    "### Step-by-Step Example\n",
    "\n",
    "For \"The cat sat on the mat\", let's say token \"cat\" has attention scores:\n",
    "```\n",
    "Raw scores:     [0.1, 0.8, 1.2, 0.3, 0.1, 0.4]\n",
    "After exp():    [1.11, 2.23, 3.32, 1.35, 1.11, 1.49]\n",
    "Sum:            11.61\n",
    "After softmax:  [0.09, 0.19, 0.29, 0.12, 0.09, 0.13]\n",
    "```\n",
    "\n",
    "**Interpretation**: \"cat\" should pay:\n",
    "- 29% attention to \"sat\" (highest score ‚Üí highest weight)\n",
    "- 19% attention to itself  \n",
    "- 13% attention to \"mat\"\n",
    "- etc.\n",
    "\n",
    "### The Attention Matrix\n",
    "\n",
    "After applying softmax to all rows, we get a **stochastic matrix**:\n",
    "\n",
    "$$\\text{Attention}_{6 \\times 6} = \\begin{bmatrix}\n",
    "\\text{The‚ÜíThe} & \\text{The‚Üícat} & \\text{The‚Üísat} & \\cdots \\\\\n",
    "\\text{cat‚ÜíThe} & \\text{cat‚Üícat} & \\text{cat‚Üísat} & \\cdots \\\\\n",
    "\\text{sat‚ÜíThe} & \\text{sat‚Üícat} & \\text{sat‚Üísat} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Properties**:\n",
    "- Each row sums to 1 (probability distribution)\n",
    "- Each entry is between 0 and 1\n",
    "- Row $i$ shows how token $i$ distributes its attention\n",
    "\n",
    "### Concrete Example: \"The cat sat on the mat\"\n",
    "\n",
    "The attention weights might reveal linguistic patterns:\n",
    "```\n",
    "         The   cat   sat   on    the   mat\n",
    "    The [0.2, 0.15, 0.1, 0.15, 0.25, 0.15]  # Articles attend to nouns\n",
    "    cat [0.1, 0.25, 0.4, 0.05, 0.05, 0.15]  # Subject attends to verb\n",
    "    sat [0.05, 0.35, 0.3, 0.1, 0.05, 0.15]  # Verb attends to subject\n",
    "    on  [0.1, 0.1, 0.15, 0.2, 0.15, 0.3]   # Preposition attends to object\n",
    "    the [0.15, 0.1, 0.1, 0.15, 0.25, 0.25]  # Article attends to noun\n",
    "    mat [0.1, 0.2, 0.15, 0.25, 0.15, 0.15]  # Object attends to preposition\n",
    "```\n",
    "\n",
    "**Key Insights**:\n",
    "- \"cat\" (row 2) has highest weight 0.4 for \"sat\" ‚Üí Subject-verb relationship\n",
    "- \"on\" (row 4) has highest weight 0.3 for \"mat\" ‚Üí Preposition-object relationship\n",
    "- Self-attention captures word importance in context\n",
    "\n",
    "### Mathematical Properties\n",
    "\n",
    "1. **Row-wise normalization**: $\\sum_{j=1}^{L} \\text{Attention}_{i,j} = 1$ for all $i$\n",
    "\n",
    "2. **Temperature effect**: Higher scores get exponentially more weight\n",
    "   - Score difference of 1 ‚Üí Weight ratio of $e ‚âà 2.7$\n",
    "   - Score difference of 2 ‚Üí Weight ratio of $e^2 ‚âà 7.4$\n",
    "\n",
    "3. **Concentration**: Softmax concentrates probability mass on highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Implement softmax to get attention weights\n",
    "# Implemented: Apply softmax to convert attention scores to attention weights\n",
    "\n",
    "def compute_attention_weights(attention_scores):\n",
    "    \"\"\"\n",
    "    Convert attention scores to attention weights using softmax.\n",
    "    \n",
    "    Args:\n",
    "        attention_scores: Attention scores (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Implementation follows: weights = softmax(scores)\n",
    "    \n",
    "    # ===== REQUIRED IMPLEMENTATION (GRADED) =====\n",
    "    # TODO: Apply softmax along the last dimension (key positions)\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    # ===== END REQUIRED SECTION =====\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# Test implementation\n",
    "attention_weights = compute_attention_weights(attention_scores)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Sum of weights for first query position: {attention_weights[0, 0, :].sum():.4f}\")\n",
    "print(f\"Sum of weights for second query position: {attention_weights[0, 1, :].sum():.4f}\")\n",
    "print(\"All attention weights should sum to 1.0 for each query position\")\n",
    "print(\"Successfully computed attention weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "visualize_attention_weights(attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù REQUIRED: Section 4: Value Aggregation\n",
    "\n",
    "## The Intuition: Gathering Information\n",
    "\n",
    "Now comes the payoff! We've decided **where** to look (attention weights), now we need to **gather** the actual information from those locations. This is like:\n",
    "\n",
    "- **Step 3 of our library analogy**: After deciding which books are most relevant (attention weights), you actually **read and combine** information from those books (values)\n",
    "- **Weighted averaging**: Instead of reading all books equally, you focus more on the most relevant ones\n",
    "\n",
    "### The Mathematical Operation\n",
    "\n",
    "$$\\text{Output} = \\text{Attention Weights} \\times V$$\n",
    "\n",
    "More precisely:\n",
    "$$\\text{Output}_i = \\sum_{j=1}^{L} \\text{Attention}_{i,j} \\times V_j$$\n",
    "\n",
    "Where:\n",
    "- $\\text{Output}_i$: The new representation for token $i$\n",
    "- $\\text{Attention}_{i,j}$: How much token $i$ attends to token $j$  \n",
    "- $V_j$: The value vector for token $j$\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "For each token, we create a **personalized summary** of the entire sequence:\n",
    "\n",
    "**For token \"cat\" in \"The cat sat on the mat\":**\n",
    "```\n",
    "Original value of \"cat\": [cat's semantic features]\n",
    "After attention:         [0.1√ó\"The\" + 0.25√ó\"cat\" + 0.4√ó\"sat\" + 0.05√ó\"on\" + 0.05√ó\"the\" + 0.15√ó\"mat\"]\n",
    "```\n",
    "\n",
    "**The result**: \"cat\" now contains:\n",
    "- 40% of \"sat\"'s information (strong subject-verb connection)  \n",
    "- 25% of its own information (self-context)\n",
    "- 15% of \"mat\"'s information (object relationship)\n",
    "- Small amounts from other tokens\n",
    "\n",
    "### What Makes This Powerful?\n",
    "\n",
    "1. **Contextualization**: Each token's representation now includes relevant context\n",
    "2. **Selective Focus**: More important relationships get more weight\n",
    "3. **Information Flow**: Semantic information flows from keys to queries through values\n",
    "\n",
    "### Tensor Shape Analysis\n",
    "\n",
    "For \"The cat sat on the mat\" (6 tokens, $d_k = 64$):\n",
    "\n",
    "1. **Attention weights**: `(1, 6, 6)` - How each token attends to every other token\n",
    "2. **Values (V)**: `(1, 6, 64)` - 64-dimensional value vector for each token\n",
    "3. **Output**: `(1, 6, 64)` - 64-dimensional attended representation for each token\n",
    "\n",
    "**Matrix multiplication**:\n",
    "- Row $i$ of attention weights: `(1, 6)` - attention distribution for token $i$\n",
    "- Full values matrix: `(6, 64)` - all value vectors\n",
    "- Result for token $i$: `(1, 64)` - weighted combination of all value vectors\n",
    "\n",
    "### The Complete Information Flow\n",
    "\n",
    "Let's trace what happens to the word \"cat\":\n",
    "\n",
    "1. **Query Creation**: \"cat\" ‚Üí Query vector (what information does \"cat\" need?)\n",
    "2. **Attention Computation**: Query compared to all Key vectors ‚Üí Attention scores  \n",
    "3. **Softmax**: Attention scores ‚Üí Attention weights (probability distribution)\n",
    "4. **Value Aggregation**: Attention weights √ó Value vectors ‚Üí Final representation\n",
    "\n",
    "**The result**: The new representation of \"cat\" contains:\n",
    "- Its original semantic information\n",
    "- **Plus** contextual information from \"sat\" (it performs this action)\n",
    "- **Plus** contextual information from \"mat\" (location relationship)  \n",
    "- **Plus** smaller amounts from other tokens\n",
    "\n",
    "### Why Values Are Different From Keys?\n",
    "\n",
    "- **Keys**: Optimized to be \"found\" by queries (searchable representations)\n",
    "- **Values**: Optimized to provide useful information (retrievable content)\n",
    "- **Analogy**: Keys are like book titles/tags, Values are like book contents\n",
    "\n",
    "### Example: Attention in Practice\n",
    "\n",
    "For \"The cat sat on the mat\":\n",
    "\n",
    "**Before attention**: Each word has only its own meaning\n",
    "- \"cat\" ‚Üí [animal, feline, small, ...]\n",
    "- \"sat\" ‚Üí [action, past tense, positioning, ...]\n",
    "\n",
    "**After attention**: Each word incorporates contextual information  \n",
    "- \"cat\" ‚Üí [animal, feline, **performed sitting**, **on furniture**, ...]\n",
    "- \"sat\" ‚Üí [action, **done by cat**, past tense, **on mat**, ...]\n",
    "\n",
    "### The Output: Contextualized Representations\n",
    "\n",
    "The final output is a set of **contextualized embeddings** where each token's representation has been enriched with relevant information from the entire sequence, weighted by attention.\n",
    "\n",
    "This forms the foundation for:\n",
    "- **Language understanding**: Words understand their context\n",
    "- **Compositionality**: Meaning emerges from relationships  \n",
    "- **Long-range dependencies**: Distant words can influence each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_values(attention_weights, V):\n",
    "    \"\"\"\n",
    "    Aggregate value vectors using attention weights.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: Attention weights (batch_size, seq_len, seq_len)\n",
    "        V: Value tensor (batch_size, seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attended output (batch_size, seq_len, d_v)\n",
    "    \"\"\"\n",
    "    # ===== REQUIRED IMPLEMENTATION (GRADED) =====\n",
    "    # TODO: Multiply attention weights with value vectors\n",
    "    # attention_weights: (batch_size, seq_len, seq_len)\n",
    "    # V: (batch_size, seq_len, d_v)\n",
    "    # output: (batch_size, seq_len, d_v)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    # ===== END REQUIRED SECTION =====\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test implementation\n",
    "attended_output = aggregate_values(attention_weights, V)\n",
    "print(f\"Attended output shape: {attended_output.shape}\")\n",
    "print(f\"Original V shape: {V.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(\"Successfully computed attended values!\")\n",
    "print(\"Each token now has a contextualized representation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "visualize_attended_values(attended_output, attention_weights, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Complete Attention Mechanism\n",
    "\n",
    "You did it, great job!\n",
    "Now let's put it all together into a complete attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_mechanism(embeddings, d_k=64):\n",
    "    \"\"\"\n",
    "    Complete attention mechanism implementation.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Input embeddings (batch_size, seq_len, d_model)\n",
    "        d_k: Dimension for Q, K, V projections\n",
    "    \n",
    "    Returns:\n",
    "        output: Attended output (batch_size, seq_len, d_k)\n",
    "        attention_weights: Attention weights for visualization\n",
    "    \"\"\"\n",
    "    # Create Q, K, V projections\n",
    "    Q, K, V = create_qkv_projections(embeddings, d_k=d_k)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    attention_scores = compute_attention_scores(Q, K)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = compute_attention_weights(attention_scores)\n",
    "    \n",
    "    # Aggregate values using attention weights\n",
    "    output = aggregate_values(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test complete implementation\n",
    "final_output, final_attention_weights = attention_mechanism(embeddings)\n",
    "print(f\"Final output shape: {final_output.shape}\")\n",
    "print(f\"Final attention weights shape: {final_attention_weights.shape}\")\n",
    "print(f\"Input embedding dim: {embeddings.shape[-1]}, Output dim: {final_output.shape[-1]}\")\n",
    "print(\"Complete attention mechanism implemented successfully!\")\n",
    "print(\"The attention mechanism has transformed static embeddings into contextualized representations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of your implementation\n",
    "evaluation_results = evaluate_attention_output(final_output, final_attention_weights, embeddings)\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# END OF GRADED ASSIGNMENT\n",
    "# HERE BEGINS PART B: DEEPER UNERSTANDING\n",
    "\n",
    "You are officially done with the mandatory lesson and first of all, awesome work completing sections 1-4 and thank you for taking my lesson!\n",
    "It's difficult to cover topics as large as  *Attention & Transformers* in such a short period of time.\n",
    "I have therefore decided to add **optional** content below for those that want to keep learning.\n",
    "This requires no further interaction from the student - simply sit back, relax and run some cells.\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Section 5: Transformer Model Comparison\n",
    "\n",
    "## From Educational Implementation to Production Reality\n",
    "\n",
    "Now that you understand how attention works from first principles, let's see how your implementation compares to real-world production transformers. This section bridges the gap between learning and practical application.\n",
    "\n",
    "### The Scale Gap: Education vs Production\n",
    "\n",
    "Your reference implementation was designed for **clarity and understanding**:\n",
    "- 64-dimensional embeddings (easy to visualize and debug)\n",
    "- Single attention head (focus on core mechanism)\n",
    "- One attention computation (minimal complexity)\n",
    "- Educational example: \"The cat sat on the mat\"\n",
    "\n",
    "Production transformers prioritize **performance and expressiveness**:\n",
    "- 768+ dimensional embeddings (rich representation space)\n",
    "- 12+ attention heads (multiple perspectives on relationships)\n",
    "- 6-12+ layers (deep hierarchical processing)\n",
    "- Complex tokenization and vocabulary handling\n",
    "\n",
    "### Key Questions This Section Answers\n",
    "\n",
    "1. **Scale**: How much larger are production models compared to your reference?\n",
    "2. **Architecture**: What additional complexity do production models add?\n",
    "3. **Consistency**: Do production models use the same core attention mechanism?\n",
    "4. **Performance**: Why do production models need so much more complexity?\n",
    "\n",
    "### Your Learning Journey\n",
    "\n",
    "This comparison helps you:\n",
    "- **Appreciate the fundamentals**: The core mechanism remains the same\n",
    "- **Understand complexity**: See why production models are more sophisticated\n",
    "- **Bridge theory to practice**: Connect academic understanding to real applications\n",
    "- **Gain perspective**: Recognize what scales and what stays constant\n",
    "\n",
    "Let's explore these differences hands-on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER: #3\n",
    "# TODO: Compare your implementation with production transformer\n",
    "\n",
    "# Import the comparison function\n",
    "from src.model_utils import compare_attention_implementations\n",
    "\n",
    "print(\"Running side-by-side comparison...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the comprehensive comparison using our example text\n",
    "try:\n",
    "    comparison_results = compare_attention_implementations(PROMPT_EXAMPLE)\n",
    "    \n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"Reference embedding dimension: {comparison_results['comparison']['embedding_dimensions']['reference']}\")\n",
    "    print(f\"Transformer embedding dimension: {comparison_results['comparison']['embedding_dimensions']['transformer']}\")\n",
    "    print(f\"Dimension ratio (production/reference): {comparison_results['comparison']['embedding_dimensions']['ratio']:.1f}x\")\n",
    "    print(f\"Both use proper attention normalization: {comparison_results['comparison']['attention_patterns']['both_sum_to_one']}\")\n",
    "    \n",
    "    print(\"\\nKey Educational Insights:\")\n",
    "    for insight in comparison_results['educational_insights']:\n",
    "        print(f\"‚Ä¢ {insight}\")\n",
    "    \n",
    "    print(\"\\nComparison complete! Ready for visualization.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")\n",
    "    print(\"This might be due to missing model loading or implementation issues\")\n",
    "    comparison_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Visual Comparison\n",
    "\n",
    "Now that we have comparison results, let's create visualizations that show the differences between your reference implementation and the production transformer. This will help you see both the similarities and differences at a glance.\n",
    "\n",
    "### What These Visualizations Show\n",
    "\n",
    "1. **Attention Weight Heatmaps**: Side-by-side comparison of attention patterns\n",
    "2. **Embedding Dimension Comparison**: Visual representation of the scale difference\n",
    "3. **Architecture Complexity**: Comparison of model complexity metrics\n",
    "4. **Parameter Count Visualization**: Understanding the computational requirements\n",
    "\n",
    "These visualizations make abstract concepts concrete and help bridge the gap between educational simplicity and production complexity.\n",
    "\n",
    "### Interpreting the Visualizations\n",
    "\n",
    "When you look at the visualizations:\n",
    "\n",
    "1. **Attention Patterns**: Notice how both use similar probability distributions\n",
    "2. **Dimension Scale**: See the 12x size difference visually represented\n",
    "3. **Complexity**: Observe the difference between 1 head vs 12 heads\n",
    "4. **Core Mechanism**: Both use the same fundamental attention formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER: #5\n",
    "# TODO: Visualize model comparison\n",
    "\n",
    "# Import the visualization function (if not already imported)\n",
    "from src.model_utils import visualize_model_comparison\n",
    "\n",
    "print(\"Creating visual comparison of implementations...\")\n",
    "print(\"Note: This creates a comprehensive 2x2 subplot showing key differences\")\n",
    "\n",
    "# Use the visualize_model_comparison() function\n",
    "if 'comparison_results' in locals() and comparison_results is not None:\n",
    "    try:\n",
    "        visualize_model_comparison(comparison_results)\n",
    "        print(\"Visualization created successfully!\")\n",
    "        print(\"Compare the attention patterns, dimensions, and architecture complexity.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        print(\"This may be due to matplotlib backend issues in some environments\")\n",
    "else:\n",
    "    print(\"No comparison results available. Please run the comparison cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Dimension Adaptation: Bridging the Gap\n",
    "\n",
    "One challenge when working with both educational and production models is handling the dimension mismatch. Your reference uses 64D embeddings while production models use 768D+. Let's explore how to bridge this gap.\n",
    "\n",
    "### Why Dimension Adaptation Matters\n",
    "\n",
    "- **Integration**: Combining insights from both implementations\n",
    "- **Visualization**: Adapting production outputs for educational visualization\n",
    "- **Experimentation**: Testing ideas across different scales\n",
    "- **Understanding**: Seeing how dimensional choices affect model behavior\n",
    "\n",
    "### Adaptation Methods\n",
    "\n",
    "1. **Projection**: Linear transformation (learns optimal mapping)\n",
    "2. **Padding**: Adding zeros (preserves original information)\n",
    "3. **Truncation**: Simple reduction (may lose information)\n",
    "\n",
    "### TODO: Experiment with Adaptation\n",
    "\n",
    "Consider these questions as you explore:\n",
    "\n",
    "1. **Which method** do you think would work best for different use cases?\n",
    "2. **What information** might be lost when reducing dimensions?\n",
    "3. **How might** different adaptation methods affect attention patterns?\n",
    "4. **When would you** want to go from 64D to 768D vs 768D to 64D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDER: #7\n",
    "# COMPLETED: Explore dimension adaptation\n",
    "\n",
    "# Import the adapt_dimensions function\n",
    "from src.model_utils import adapt_dimensions\n",
    "\n",
    "print(\"Exploring dimension adaptation between reference and production scales...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get your reference embeddings from earlier in the notebook\n",
    "print(f\"Starting with reference embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Experiment with dimension adaptation\n",
    "methods = [\"project\", \"pad\", \"truncate\"]\n",
    "target_dim = 768  # Production transformer dimension\n",
    "\n",
    "print(f\"\\nAdapting from {embeddings.shape[-1]}D to {target_dim}D:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Loop through each method and test adaptation\n",
    "for method in methods:\n",
    "    print(f\"\\nMethod: {method.upper()}\")\n",
    "    \n",
    "    try:\n",
    "        adapted = adapt_dimensions(embeddings, target_dim, method=method)\n",
    "        print(f\"   Original: {embeddings.shape}\")\n",
    "        print(f\"   Adapted:  {adapted.shape}\")\n",
    "        print(f\"   Successfully adapted using {method} method\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with {method} method: {e}\")\n",
    "\n",
    "# Test reverse adaptation (Production ‚Üí Reference)\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Testing reverse adaptation (Production ‚Üí Reference)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "reference_dim = 64\n",
    "# Create a simulated production tensor\n",
    "production_tensor = torch.randn(1, 6, 768)\n",
    "print(f\"Production tensor shape: {production_tensor.shape}\")\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nReverse Method: {method.upper()}\")\n",
    "    try:\n",
    "        reverse_adapted = adapt_dimensions(production_tensor, reference_dim, method=method)\n",
    "        print(f\"   Production: {production_tensor.shape}\")\n",
    "        print(f\"   Adapted:    {reverse_adapted.shape}\")\n",
    "        print(f\"   Successfully adapted from 768D to 64D using {method}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with reverse {method}: {e}\")\n",
    "\n",
    "print(f\"\\nREFLECTION:\")\n",
    "print(\"1. Project method creates learned transformations\")\n",
    "print(\"2. Pad method preserves original information but adds zeros\")\n",
    "print(\"3. Truncate method may lose information but is simple\")\n",
    "print(\"4. Each method has different computational and information trade-offs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ The Big Picture: How Attention Transforms Understanding\n",
    "\n",
    "You have successfully implemented the attention mechanism! Let's connect all the pieces to see the complete picture.\n",
    "\n",
    "### The Four-Step Journey\n",
    "\n",
    "**The attention mechanism solves a fundamental problem**: How can each word in a sentence understand and incorporate information from all other words?\n",
    "\n",
    "1. **Linear Projections (Q, K, V)**: Create three different \"views\" of each word\n",
    "   - Transform static embeddings into dynamic, task-specific representations\n",
    "   - Enable words to express what they need, what they offer, and what they contain\n",
    "\n",
    "2. **Scaled Dot-Product**: Measure compatibility between information needs and offerings\n",
    "   - Quantify relationships through geometric similarity (dot products)\n",
    "   - Scale to maintain stable gradients for effective learning\n",
    "\n",
    "3. **Softmax Normalization**: Convert compatibility into attention allocation  \n",
    "   - Create probability distributions for interpretable attention weights\n",
    "   - Ensure each word allocates exactly 100% of its attention across all positions\n",
    "\n",
    "4. **Value Aggregation**: Gather and combine relevant information\n",
    "   - Perform weighted averaging based on attention decisions\n",
    "   - Create contextualized representations that incorporate global information\n",
    "\n",
    "### The Transformation: From Static to Dynamic\n",
    "\n",
    "**Before Attention** (Static embeddings):\n",
    "```\n",
    "\"The\" ‚Üí [article, definite, ...]\n",
    "\"cat\" ‚Üí [animal, feline, small, ...]  \n",
    "\"sat\" ‚Üí [action, past, positioning, ...]\n",
    "\"on\"  ‚Üí [preposition, spatial, ...]\n",
    "\"the\" ‚Üí [article, definite, ...]\n",
    "\"mat\" ‚Üí [object, flat, surface, ...]\n",
    "```\n",
    "\n",
    "**After Attention** (Contextualized representations):\n",
    "```\n",
    "\"The\" ‚Üí [article, **refers to cat**, definite, ...]\n",
    "\"cat\" ‚Üí [animal, **performs sitting**, feline, **subject role**, ...]\n",
    "\"sat\" ‚Üí [action, **done by cat**, past, **on surface**, ...]  \n",
    "\"on\"  ‚Üí [preposition, **connects cat and mat**, spatial, ...]\n",
    "\"the\" ‚Üí [article, **refers to mat**, definite, ...]\n",
    "\"mat\" ‚Üí [object, **location of sitting**, flat, **receives cat**, ...]\n",
    "```\n",
    "\n",
    "### Key Insights and Implications\n",
    "\n",
    "#### 1. **Parallel Processing**\n",
    "Unlike sequential models (RNNs), attention processes all positions simultaneously:\n",
    "- All words can attend to all other words in one pass\n",
    "- Enables parallelization and faster training\n",
    "- Captures long-range dependencies directly\n",
    "\n",
    "#### 2. **Learned Relationships**  \n",
    "The attention patterns emerge from learning, not hard-coded rules:\n",
    "- Q, K, V projections learn what relationships to look for\n",
    "- Attention weights discover syntactic and semantic patterns\n",
    "- Model learns grammar, syntax, and semantics implicitly\n",
    "\n",
    "#### 3. **Context-Dependent Meaning**\n",
    "Words develop different meanings based on context:\n",
    "- \"bank\" in \"river bank\" vs. \"savings bank\" gets different attended information\n",
    "- Same mechanism handles ambiguity resolution and context integration\n",
    "- Dynamic contextualization at every layer\n",
    "\n",
    "#### 4. **Foundation for Transformers**\n",
    "This attention mechanism is the core building block of:\n",
    "- **BERT**: Bidirectional attention for understanding\n",
    "- **GPT**: Causal (masked) attention for generation  \n",
    "- **T5**: Encoder-decoder attention for translation\n",
    "- **Vision Transformers**: Attention over image patches\n",
    "\n",
    "### Mathematical Elegance\n",
    "\n",
    "The entire mechanism can be expressed in one equation:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "This simple formula encapsulates:\n",
    "- Information retrieval (queries and keys)\n",
    "- Relevance measurement (dot products)\n",
    "- Decision making (softmax)\n",
    "- Information aggregation (weighted values)\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "1. **Theoretical Foundation**: Deep understanding of why each component is necessary\n",
    "2. **Mathematical Formulation**: Precise equations and their intuitive meanings  \n",
    "3. **Implementation Skills**: Hands-on experience building attention from scratch\n",
    "4. **Tensor Thinking**: Understanding of shapes, dimensions, and operations\n",
    "5. **Architectural Insight**: How attention enables modern language models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With this foundation, you're ready to explore:\n",
    "- **Multi-head attention**: Multiple parallel attention mechanisms\n",
    "- **Transformer architecture**: Stacking attention with feedforward layers\n",
    "- **Positional encoding**: Handling sequence order information\n",
    "- **Advanced variants**: Sparse attention, linear attention, and more\n",
    "\n",
    "**Congratulations!** You've mastered one of the most important innovations in modern AI. The attention mechanism you've implemented forms the backbone of today's most powerful language models and continues to drive breakthroughs in artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED: Explore dimension adaptation\n",
    "\n",
    "# Import the adapt_dimensions function\n",
    "from src.model_utils import adapt_dimensions\n",
    "\n",
    "print(\"Exploring dimension adaptation between reference and production scales...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get your reference embeddings from earlier in the notebook\n",
    "print(f\"Starting with reference embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Experiment with dimension adaptation\n",
    "methods = [\"project\", \"pad\", \"truncate\"]\n",
    "target_dim = 768  # Production transformer dimension\n",
    "\n",
    "print(f\"\\nAdapting from {embeddings.shape[-1]}D to {target_dim}D:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Loop through each method and test adaptation\n",
    "for method in methods:\n",
    "    print(f\"\\nMethod: {method.upper()}\")\n",
    "    \n",
    "    try:\n",
    "        adapted = adapt_dimensions(embeddings, target_dim, method=method)\n",
    "        print(f\"   Original: {embeddings.shape}\")\n",
    "        print(f\"   Adapted:  {adapted.shape}\")\n",
    "        print(f\"   Successfully adapted using {method} method\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with {method} method: {e}\")\n",
    "\n",
    "# Test reverse adaptation (Production ‚Üí Reference)\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Testing reverse adaptation (Production ‚Üí Reference)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "reference_dim = 64\n",
    "# Create a simulated production tensor\n",
    "production_tensor = torch.randn(1, 6, 768)\n",
    "print(f\"Production tensor shape: {production_tensor.shape}\")\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nReverse Method: {method.upper()}\")\n",
    "    try:\n",
    "        reverse_adapted = adapt_dimensions(production_tensor, reference_dim, method=method)\n",
    "        print(f\"   Production: {production_tensor.shape}\")\n",
    "        print(f\"   Adapted:    {reverse_adapted.shape}\")\n",
    "        print(f\"   Successfully adapted from 768D to 64D using {method}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with reverse {method}: {e}\")\n",
    "\n",
    "print(f\"\\nREFLECTION:\")\n",
    "print(\"1. Project method creates learned transformations\")\n",
    "print(\"2. Pad method preserves original information but adds zeros\")\n",
    "print(\"3. Truncate method may lose information but is simple\")\n",
    "print(\"4. Each method has different computational and information trade-offs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéâ Conclusion: Your Journey Through Attention\n",
    "\n",
    "## What You've Accomplished\n",
    "\n",
    "Congratulations! You've successfully completed a comprehensive exploration of the attention mechanism. Let's reflect on your journey:\n",
    "\n",
    "### ‚úÖ Core Implementation (Required)\n",
    "You've built the fundamental attention mechanism from scratch:\n",
    "- **Q, K, V Projections**: Created three perspectives on information\n",
    "- **Scaled Dot-Product**: Measured compatibility between queries and keys\n",
    "- **Softmax Normalization**: Converted scores to probability distributions\n",
    "- **Value Aggregation**: Combined information based on attention weights\n",
    "\n",
    "Your implementation achieved **100% on the evaluation**, demonstrating complete understanding of the core concepts.\n",
    "\n",
    "### üî¨ Deep Exploration (Optional)\n",
    "You've explored advanced topics that bridge theory to practice:\n",
    "- **Dimension Adaptation**: Understood how to scale between educational and production models\n",
    "- **Production Comparison**: Compared your implementation with DistilGPT-2\n",
    "- **Architectural Insights**: Discovered that the same mathematical foundation scales from 64D to 768D+\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The Same Math, Different Scale**: Your 64D educational implementation and production's 768D transformers use identical mathematical operations - the difference is in scale and optimization.\n",
    "\n",
    "2. **Attention is Universal**: The mechanism you implemented is the foundation of:\n",
    "   - Language models (GPT, BERT)\n",
    "   - Translation systems\n",
    "   - Computer vision transformers\n",
    "   - And many more applications\n",
    "\n",
    "3. **Understanding Fundamentals Matters**: By building attention from scratch, you now understand what's happening inside every transformer model, regardless of size.\n",
    "\n",
    "## Your Next Steps\n",
    "\n",
    "With this foundation, you're prepared to explore:\n",
    "\n",
    "### Immediate Next Steps\n",
    "- **Multi-Head Attention**: Implement parallel attention mechanisms\n",
    "- **Positional Encoding**: Add sequence order information\n",
    "- **Complete Transformer**: Build encoder-decoder architecture\n",
    "\n",
    "### Advanced Topics\n",
    "- **Fine-tuning**: Adapt pre-trained models for specific tasks\n",
    "- **Attention Variants**: Explore sparse, linear, and other efficient attention mechanisms\n",
    "- **Cross-Modal Applications**: Apply attention to vision, speech, and multimodal tasks\n",
    "\n",
    "### Practical Applications\n",
    "- Build a simple chatbot using your attention implementation\n",
    "- Create a text classifier with transformer architecture\n",
    "- Experiment with attention visualization on real texts\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "You've mastered one of the most important innovations in modern AI. The attention mechanism you've implemented is not just an academic exercise - it's the exact same mechanism (at a smaller scale) that powers ChatGPT, BERT, and other state-of-the-art models.\n",
    "\n",
    "Remember: **You now understand the core technology behind the AI revolution.**\n",
    "\n",
    "Keep exploring, keep building, and keep learning. The journey from understanding attention to building powerful AI applications is shorter than you might think!\n",
    "\n",
    "**Well done! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
